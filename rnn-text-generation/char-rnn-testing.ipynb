{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a Char-RNN text generator\n",
    "\n",
    "In this notebook we will load and test our Char-RNN text generator. This notebook has similiarities to the `char-rnn-training.ipynb` notebook, but this time there is no training loop, we are just loading in a model and generating from it. This code is heavily modified from the repo: https://github.com/nikhilbarhate99/Char-RNN-PyTorch\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are different implementations for storing and processing data on different kinds of computer hardware. By default, all computers will work by training and running neural networks on the Central Processing Unit (CPU), which we can specify with `'cpu'`. \n",
    "\n",
    "If you have an NVIDIA Graphics Processing Unit (GPU) (and you have installed CUDA correctly and the correct version of PyTorch), then you can use the flag `'gpu'` which will make training your neural networks **much faster**. Most of you won't have powerful NVIDIA GPU's in yor laptops however. Don't worry if you don't, the notebooks we are using in this class will be designed to work on laptop CPU's. \n",
    "\n",
    "If you have an M1 or M2 processor on a Mac then you can use the device `'mps'` which will run on Apples accelerated Metal Performance Shaders (MPS) for potentially faster and more powerful training (though sometimes running on CPU can be faster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters\n",
    "\n",
    "This is where we specify our *hyperparameters*. We have less hyperparameters this time as we are dont need any of the training parameters. The `hidden_size` and `num_layers` parameters need to be the same as was set when the model was trained in the other notebook.\n",
    "\n",
    "The temperature parameter can be used to control how random or conservative our precited characters will be. If we have a low temeprature (below 1) we will more often than not pick the most likely character. If the temperature is higher (than 1) our generated sequences will be more random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512   # size of hidden state\n",
    "num_layers = 3      # number of layers in LSTM layer stack\n",
    "gen_seq_len = 100   # length of LSTM sequence\n",
    "temperature = 1     # how random do we want our predictions to be\n",
    "load_path = \"char_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network \n",
    "\n",
    "Here we define our network the same. This code must be the same as the code used in the training notebook where we saved the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_batch, hidden_state):\n",
    "        embedding = self.embedding(input_batch)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we will create an instantiation of our network `rnn`. We also need to define out loss function `loss_fn` and our `optimiser`, which is used to make make changes to the neural network weights in training. We have to make our data variable a PyTorch `tensor`. This is data type that we have to use with PyTorch so that our neural networks can read and process the data correctly. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) have been designed to work in almost exactly the same way as [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Load char_to_ix and ix_to_char dictionaries from checkpoint file\n",
    "char_to_ix = checkpoint['char_to_ix']\n",
    "ix_to_char = checkpoint['ix_to_char']\n",
    "\n",
    "# Calculate vocab size\n",
    "vocab_size = len(char_to_ix)\n",
    "\n",
    "# Instantiate RNN\n",
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Load model weights from checkpoint file \n",
    "rnn.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_state = None\n",
    "    \n",
    "    #Pick a random starting character\n",
    "    random_start = np.array(random.choice(list(char_to_ix.values())))\n",
    "    \n",
    "    # Convert to PyTorch Tensor\n",
    "    input_seq = torch.tensor(random_start, dtype=torch.int64)\n",
    "    \n",
    "    # Change dimensionality of tensor for PyTorch compatibility\n",
    "    # For more info on this function see: https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n",
    "    input_seq = input_seq.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Iterate over our sequence length\n",
    "    for i in range(gen_seq_len):\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "        \n",
    "        # Construct categorical distribution and sample a character\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output / temperature)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        # Print the sampled character\n",
    "        print(ix_to_char[index.item()], end='')\n",
    "        \n",
    "        # Next input is current output\n",
    "        input_seq[0][0] = index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map string to indexes\n",
    "\n",
    "Lets write a function where we can manually create our own starting sequence. We will take a string and use our `char_to_ix` dictionary to get the mapped numerical values. It is important to remember that **only the characters in the original dataset** will be able to be **mapped into the index values for the model**. Try printing out `char_to_ix` to see all the available characters. Any characters not in the original data will unfortunately be skipped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_str_to_ix(input_str):\n",
    "    index_list = []\n",
    "    for char in input_str:\n",
    "        ix = char_to_ix.get(char, None)\n",
    "        if ix is not None:\n",
    "            index_list.append(ix)\n",
    "        else:\n",
    "            print(f'The char {char} is not in the dictionary')\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define new starting string\n",
    "\n",
    "Now lets create our index list and convert it to a numpy array then pytorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = 'cci'\n",
    "index_list = map_str_to_ix(input_str)\n",
    "print(f'Our list is: {index_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate from randomly created starting sequence\n",
    "\n",
    "Now lets have a go at generating from our own sequence. We need to have two loops here. The first passes each character into the model to update the **hidden state**, here we are not doing anything with the models predictions, just *conditioning* the model on our sequence. Once the model is conditioned on the sequence then we can start to make new generations from it in the second loop.\n",
    "\n",
    "How do these predictions compare to the random generations? What happens when you put in a starting sequence that is very different to the original data? Try [changing the temperature parameter](#set-hyperparameters) to see how the effects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_state = None\n",
    "\n",
    "    for ix in index_list:\n",
    "        \n",
    "        # Print current input sequence\n",
    "        print(ix_to_char[ix], end='')\n",
    "\n",
    "        #Pick a random starting character\n",
    "        current_ix = np.array(ix)\n",
    "        \n",
    "        # Convert to PyTorch Tensor\n",
    "        input = torch.tensor(current_ix, dtype=torch.int64)\n",
    "        \n",
    "        # Change dimensionality of tensor for PyTorch compatibility\n",
    "        # For more info on this function see: https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n",
    "        input = input.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Condition the model on starting sequence\n",
    "        output, hidden_state = rnn(input, hidden_state)\n",
    "\n",
    "\n",
    "    # Iterate over our sequence length\n",
    "    for i in range(gen_seq_len):\n",
    "        # Forward pass\n",
    "        output, hidden_state = rnn(input, hidden_state)\n",
    "        \n",
    "        # Construct categorical distribution and sample a character\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output / temperature)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        # Print the sampled character\n",
    "        print(ix_to_char[index.item()], end='')\n",
    "        \n",
    "        # Next input is current output\n",
    "        input[0][0] = index.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
