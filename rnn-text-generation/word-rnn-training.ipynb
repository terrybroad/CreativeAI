{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a text generator with Word-RNN \n",
    "\n",
    "In this notebook we will look at how to train a Recurrent Neural Network (RNN) to model sequence of words (Word-RNN). There is a very similiar notebook to this one that generates text based on characters instead of words. Run both of them to compare different approaches. This code is heavily modified from the repo: https://github.com/nikhilbarhate99/Char-RNN-PyTorch\n",
    "\n",
    "To do this we will use the library [PyTorch](https://pytorch.org/), a library for building and training neural networks in Python. This is the first time we are using this library and looking at how to build and train neural networks, but it won't be the last! We will be spending a lot of time over the next couple of terms looking at code that looks a lot like this. It may look quite unfamiliar at first, but over time you will get used to working with and altering this kind of code. \n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there are different implementations for storing and processing data on different kinds of computer hardware. By default, all computers will work by training and running neural networks on the Central Processing Unit (CPU), which we can specify with `'cpu'`. \n",
    "\n",
    "If you have an NVIDIA Graphics Processing Unit (GPU) (and you have installed CUDA correctly and the correct version of PyTorch), then you can use the flag `'gpu'` which will make training your neural networks **much faster**. Most of you won't have powerful NVIDIA GPU's in yor laptops however. Don't worry if you don't, the notebooks we are using in this class will be designed to work on laptop CPU's. \n",
    "\n",
    "If you have an M1 or M2 processor on a Mac then you can use the device `'mps'` which will run on Apples accelerated Metal Performance Shaders (MPS) for potentially faster and more powerful training (though sometimes running on CPU can be faster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters\n",
    "\n",
    "This is where we specify our *hyperparameters*. This is where we set the parameters that determine the size of our neural network (`num_layers`,`hidden_size` and the `vocab_size`), how long we train the network for (`num_steps` and `step_len`) and how aggressively we train the network (the learning rate `lr`).  \n",
    "\n",
    "`load_chk` is a boolean that determines whether we start training from the weights of an already trained model or start from scratch. If this is true, then the path to a model file should be specified in `load_path`. If you change the dataset (and have a different vocabulary size) or make changes to any other parts of the model between saving and loading a model then this will not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512   # size of hidden state\n",
    "batch_size = 100    # size of the batch used for training\n",
    "step_len = 200      # number of training samples in each stem\n",
    "num_layers = 3      # number of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "num_steps = 100     # max number of training steps\n",
    "gen_seq_len = 50    # length of generated sequence\n",
    "load_chk = False    # load in pre-trained checkpoint for training\n",
    "save_path = \"word_rnn_model.pt\"\n",
    "# load_path = \"word_rnn_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load multiple text files\n",
    "\n",
    "This function should now look familiar. We will use it to load in our Nursery Rhymes dataset. As we are loading in all of the files into one string variable we need a way to determine where one rhyme ends and another begins. We will do this by adding in the word `EOF` (End of File) which can will the model to represent where one rhyme ends and another begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_text_files_in_folder(path, max_files = 10000):\n",
    "    corpus = ''\n",
    "    # Find all files in the folder or subfolders\n",
    "    for root, _, files in os.walk(path):\n",
    "        for i, file in enumerate(files):\n",
    "            # If the file is a text file\n",
    "            if file.endswith(\".txt\") and i <= max_files:\n",
    "                # Open the file and add the text to the corpus\n",
    "                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    # Add text from file\n",
    "                    corpus += text\n",
    "                    # Add 'End of File' between documents\n",
    "                    corpus += '\\n EOF \\n'\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our data\n",
    "\n",
    "Now lets load in our data. We are going to use a dataset of pokemon names to start off with. We are going to use the [Python set data structure](https://www.w3schools.com/python/python_sets.asp) to find all of the unique words in our text. We will first use the `split()` function to split our corpus into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/nursery-rhymes\"\n",
    "corpus = load_all_text_files_in_folder(data_path)\n",
    "words = sorted(list(set(corpus.split())))\n",
    "data_size, vocab_size = len(corpus.split()), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create two dictionaries, one will give use a mapping from words to their indexes, and indicies to the respective words they represent. \n",
    "\n",
    "Our variable `data` is going to be a mapping of our text `corpus`, but a list of the numerical index values of each character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = { w:i for i,w in enumerate(words) }\n",
    "ix_to_word = { i:w for i,w in enumerate(words) }\n",
    "\n",
    "data = corpus.split()\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = word_to_ix[ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network \n",
    "\n",
    "This is where we define our neural network. We define a neural network as a `class`. Classes can have **functions** and ****variables** that it owns. Classes are a fundamental part of [object-orientated programming in Python](https://www.w3schools.com/python/python_classes.asp) (and many other programming languages). In this case we are building our class `RNN` by [inheriting from the base class](https://www.w3schools.com/python/python_inheritance.asp) for a neural network in PyTorch called `nn.Module`. \n",
    "\n",
    "We need to define two functions for a PyTorch neural network class. The `__init__` function gets called when we create the class, here we create and set the variables that our network needs (such as the all of layers and other things we may need to keep track of). In this function the first thing we need to call is the `super` function that will call the `__init__` function of the base class we are inheriting from. \n",
    "\n",
    "The other function we need to define for a PyTorch neural networks is the function `forward` function. This defines what happens when we do a forward pass with our network (taking data as an input and giving somethign else as an output). Because this is a recurrent neural network, our network needs to take as input both the data and the hidden state (the previous iteration) of the model. This function also outputs the hidden state so that we can pass it back into the function at a later iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_batch, hidden_state):\n",
    "        embedding = self.embedding(input_batch)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up network and optimiser\n",
    "\n",
    "Here we will create an instantiation of our network `rnn`. We also need to define out loss function `loss_fn` and our `optimiser`, which is used to make make changes to the neural network weights in training. We have to make our data variable a PyTorch `tensor`. This is data type that we have to use with PyTorch so that our neural networks can read and process the data correctly. [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) have been designed to work in almost exactly the same way as [numpy arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of indexes that can be valid starting points for training\n",
    "index_list = list(range(0, len(data) - step_len - 1))\n",
    "\n",
    "# Conver data to torch tensor\n",
    "data = torch.tensor(data).to(device)\n",
    "data = torch.unsqueeze(data, dim=1)\n",
    "\n",
    "# Create RNN class\n",
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "# Load in pretrained model if specified\n",
    "if load_chk:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    rnn.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data randomly\n",
    "\n",
    "This function will allow us to do random sampling of the dataset in training. When we train neural networks we almost always train on **random batches of data**. In training we process lots of data samples at the same time (with lots of copies of the neural network), and then average our loss over all of the data samples and update the weights accordingly. This helps with the *regularisation* of the network, and makes training much more stable. \n",
    "\n",
    "The number of data samples we have in each mini-batch is defined by the `batch_size`, generally speaking the more batches you can use the better (though there are exceptions to this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch_indicies(index_list, batch_size):\n",
    "    # Get a batch of indicies to sample our data from\n",
    "    input_batch_indicies = torch.tensor(np.array(random.sample(index_list, batch_size)))\n",
    "    # Offset indicies for target batch by one\n",
    "    target_batch_indicies = input_batch_indicies + 1\n",
    "    return input_batch_indicies, target_batch_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "\n",
    "Here we have all the code we need for our **training loop**. Here we are looping through our number of training steps (defined in `num_steps`). Each step we will sample a random section of a dataset that will cycle for `step_len` training iterations. In some code bases, you will see code that cycles through *epochs* (complete cycles of the dataset). We aren't doing that here as the training time can vary drastically based on how much data is in your dataset. \n",
    "\n",
    "After each iteration the weights of the model will be saved to the file `word_rnn_model.pt`. Sometimes people will save different versions of the file after a set number of step (i.e. `word_rnn_model_50.pt` or `word_rnn_model_100.pt`), but this will fill up your drive very quickly! For now we will just keep overwriting the same file after each iteration. \n",
    "\n",
    "If you are happy with the outputs or need to stop the code running for whatever reason, you can just kill the cell and your progress will be saved. This can be loaded into the notebook `text-generation-with-char-rnn-test.ipynb` to be used for code that just performs generation. \n",
    "\n",
    "The most important parts of any training code are the **forward pass** (where we process our data with our neural network), calculating the **loss function** where evaluate how well our model has performed against the real value of the data. Then we have to **update the weights of the neural network**. This is done by calling `loss.backward()` followed by `optimizer.step()`.\n",
    "\n",
    "After each iteration of the code we generate a new sequence with the network so we can see how the network is improving during training. This is done without gradient tracking `with torch.no_grad():` (gradient tracking is what is used for training and calculating how much to adjust the weights of our network by at each step). \n",
    "\n",
    "This will probably all look quite complicated and hard to understand first time around. **That is ok!** Over time as you see and work with more and more code that looks like this you will start getting used to it and feel more confident in adapting, changing and writing this kind of code yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the number of steps defined earlier\n",
    "for step in range(1, num_steps):\n",
    "    \n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "    rnn.zero_grad()\n",
    "    train_batch_indicies, target_batch_indicies = get_training_batch_indicies(index_list, batch_size)\n",
    "\n",
    "    # Cycle through for a set number of consecutive iterations in the data\n",
    "    for i in range(step_len):\n",
    "        # Extract data batches from indicies\n",
    "        input_batch = data[train_batch_indicies].squeeze()\n",
    "        target_batch = data[target_batch_indicies].squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        # The following code is the same as calling rnn.forward(input_batch, hidden_state)\n",
    "        output, hidden_state = rnn(input_batch, hidden_state)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, target_batch)\n",
    "        running_loss += loss.item() / step_len\n",
    "        \n",
    "        # Update weights of neural network\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Increment batch coordinates by 1\n",
    "        train_batch_indicies = train_batch_indicies + 1\n",
    "        target_batch_indicies = target_batch_indicies + 1\n",
    "\n",
    "        \n",
    "    # Print loss\n",
    "    print('\\n'+'-'*75)\n",
    "    print(f\"\\nStep: {step} Loss: {running_loss}\")\n",
    "\n",
    "    # Create a dictionary for saving the model and data mappings\n",
    "    save_dict = {}\n",
    "    # Add the model weight parameters as a dictionary to our save_dict\n",
    "    save_dict['state_dict'] = rnn.state_dict()\n",
    "    # Add the idx_to_word and word_to_idx dicts to our save_dict\n",
    "    save_dict['ix_to_word'] = ix_to_word\n",
    "    save_dict['word_to_ix'] = word_to_ix\n",
    "    # Save the dictionary to a file\n",
    "    torch.save(save_dict, save_path)\n",
    "\n",
    "    # Now lets generate a random generated text sample to print out,\n",
    "    # we will do this without gradient tracking as we are not training\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Take a random index and reset the hidden state of the model\n",
    "        rand_index = np.random.randint(data_size-1)\n",
    "        input_batch = data[rand_index : rand_index+1]\n",
    "        hidden_state = None\n",
    "        \n",
    "        # Iterate over our sequence length\n",
    "        for i in range(gen_seq_len):\n",
    "            # Forward pass\n",
    "            output, hidden_state = rnn(input_batch, hidden_state)\n",
    "            \n",
    "            # Construct categorical distribution and sample a character\n",
    "            output = F.softmax(torch.squeeze(output), dim=0)\n",
    "            dist = Categorical(output)\n",
    "            index = dist.sample()\n",
    "            \n",
    "            # Print the sampled character\n",
    "            print(ix_to_word[index.item()], end=' ')\n",
    "            \n",
    "            # Next input is current output\n",
    "            input_batch[0][0] = index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "\n",
    "First do these tasks in order before moving onto the bonus tasks:\n",
    "\n",
    "**Task 1:** Run the code cells and train a model on the Nursery Rhymes names. Is it possble after training to generate plausible looking nursery rhymes. If the model training has finished and you are not happy with the results you can set `load_chk` to `True` in [the cell that defines the hyperparameters](#set-hyperparameters) and load in a model from the variable `load_path`.\n",
    "\n",
    "**Task 2:** Compare the code in this notebook to the Char-RNN training (`char-rnn-training.ipynb`). How does the code differ? What elements are the same between the notebooks.\n",
    "\n",
    "**Task 3:** Load your trained model into the notebook `word-rnn-testing.ipynb` to have a go at more generation with the model you have trained. \n",
    "\n",
    "**Task 4:** Can you adapt this code to load in another dataset? Have a look at the code in `text-generation-with-markov-chains.ipynb` from Week 5 to use functions to load in datasets in different formats. Can make a direct comparison between a Char-RNN model and a Word-RNN model trained on the same dataset? \n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "These bonus tasks can be done in any order.\n",
    "\n",
    "After each training run you may want to rename the checkpoint files from each training run so you can keep them for comparison later.\n",
    "\n",
    "**Task A:** Try changing some of the other hyperparameters in [the cell that defines the hyperparameters](#set-hyperparameters). Such as `hidden_size` `batch_size`, `num_layers` or `lr`. Restart the kernel and run the training again. \n",
    "\n",
    "**Task B:** Try changing the optimiser used [the cell where the network and optimiser are instantiated](#setting-up-network-and-optimiser) to one of [the other optimisers available in PyTorch](https://pytorch.org/docs/stable/optim.html), such as stochastic gradient descent (SGD) or Adagrad. Restart the kernel and run the training again. \n",
    "\n",
    "**Task C:** Try changing the type of layer used [in the RNN network](#defining-the-network) from LSTM to a [vanilla RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) or a [Gated Recurrent Unit](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) (GRU). Restart the kernel and run the training again. \n",
    "\n",
    "**Task D:** Can you use stop words or do some other kinds of cleaning or normalisation to the dataset to improve or edit the quality of the generated results? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
