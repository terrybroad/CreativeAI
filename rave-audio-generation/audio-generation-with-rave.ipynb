{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio generation with RAVE\n",
        "\n",
        "RAVE is a Real-time Audio Variational autoEncoder (https://github.com/acids-ircam/RAVE) released by Caillon and Esling (ACIDS IRCAM) in November 2021. You can read the paper here: https://arxiv.org/abs/2111.05011. RAVE is a particularly light model that allows generating audio in real-time in the CPU and even in embedded systems with low computational power, such as Raspberry Pi (here is a video: https://youtu.be/jAIRf4nGgYI). Still, training this model is computationally expensive: in the original paper, they used 3M steps, which took six days on a TITAN V GPU. \n",
        "\n",
        "In this notebook we will see how to we can generate audio with pre-trained RAVE models, including unconditional generation (generating from latent z with the decoder), timbre transfer (passing exisiting audio through the encoder and decoder model to alter the timbre of the sound), and do some simple latent space manipulation.\n",
        "\n",
        "If you want to train your own model you will need a GPU to do it effectively (hence we are not doing it as a class activity). To train a RAVE model on a cloud based GPU see [this colab notebook for training your own RAVE model on a custom audio dataset](https://colab.research.google.com/drive/1ih-gv1iHEZNuGhHPvCHrleLNXvooQMvI?usp=sharing). Otherwise [the offical RAVE github page](https://github.com/acids-ircam/RAVE) has instructions for training a RAVE model locally.\n",
        "\n",
        "\\* If you are interested in using RAVE for performing, the real-time implementation runs in MaxMSP and can be downloaded here: https://github.com/acids-ircam/nn_tilde\n",
        "\n",
        "This notebook is adapted from a notebook originally created by [Teresa Pelinski](https://teresapelinski.com/), that was based on [this RAVE generation notebook](https://colab.research.google.com/github/hdparmar/AI-Music/blob/main/Latent_Soundings_workshop_RAVE.ipynb)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installs and imports\n",
        "\n",
        "THere are a couple of python packages that you will need to download manually the first time you run this. Uncomment and run the next lines in the cell. \n",
        "- `wget` for downloading files over the internet\n",
        "- `acids-ircam` for downloading and running rave models\n",
        "  \n",
        "As well as some additional software:\n",
        "- `ffmpeg` for transcoding audio and video (and all sorts of other useful things! see https://ffmpeg.org/)\n",
        "\n",
        "Once you have installed these you can re-comment them so they don't run again in future (You can use the shortcut: CMD+/ (mac) or CTRL+/ (windows) to do this for multiple lines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --quiet wget\n",
        "# !pip install --quiet acids-rave # --quiet avoids long outputs. if you get any errors, remove --quiet\n",
        "# !yes|conda install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import wget\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import librosa as li\n",
        "import soundfile as sf\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import floor\n",
        "from scipy import signal\n",
        "\n",
        "from src.latent_util import create_latent_interp, clamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_dim = 8 \n",
        "# sample_rate = 48000 # sample rate of the audio\n",
        "sample_rate = 44100 # sample rate of the audio"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download pretrained models\n",
        "Some info on the pretrained models is available here: https://acids-ircam.github.io/rave_models_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pt_path = \"../rave_models\" # folder where pretrained models will be downloaded\n",
        "if not os.path.exists(pt_path): # create the folder if it doesn't exist\n",
        "    os.mkdir(pt_path)\n",
        "    \n",
        "def bar_progress(current, total, width=80): # progress bar for wget\n",
        "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "    # Don't use print() as it will print in new line every time.\n",
        "    sys.stdout.write(\"\\r\" + progress_message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "pretrained_models = [\"vintage\", \"percussion\", \"VCTK\"] # list of available pretrained_models to download in https://acids-ircam.github.io/rave_models_download (you can select less if you want to spend less time on this cell)\n",
        "\n",
        "for model in pretrained_models: # download pretrained models and save them in pt_path\n",
        "    if not os.path.exists(os.path.join(pt_path, f\"{model}.ts\")): # only download if not already downloaded\n",
        "        print(f\"Downloading {model}.ts...\")\n",
        "        wget.download(f\"https://play.forum.ircam.fr/rave-vst-api/get_model/{model}\",f\"{pt_path}/{model}.ts\", bar=bar_progress)\n",
        "    else:\n",
        "        print(f\"{model}.ts already downloaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model\n",
        "\n",
        "Let us load in one of the models that we have downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_path = \"generated\" # folder where generated audio will be saved\n",
        "if not os.path.exists(generated_path): # create the folder if it doesn't exist\n",
        "    os.mkdir(generated_path)\n",
        "    \n",
        "pretrained_model = \"vintage\" # select the pretrained model to use\n",
        "\n",
        "model = torch.jit.load(f\"{pt_path}/{pretrained_model}.ts\" ).eval() # load model\n",
        "torch.set_grad_enabled(False) # disable gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random generation\n",
        "\n",
        "In the next code cell we will see how to sample randomly different points in the RAVE latent space and concatonate them into an audio clip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_clips = []\n",
        "for i in range(100):\n",
        "    # Randomly sample latent space\n",
        "    z = torch.randn(1,latent_dim,1)\n",
        "    \n",
        "    # Generate audio clip and append to list\n",
        "    gen_audio_clip = model.decode(z)\n",
        "    gen_audio_clip = gen_audio_clip.reshape(-1).cpu().numpy()\n",
        "    generated_clips.append(gen_audio_clip)\n",
        "\n",
        "# Concatonate list of audio clips into one array\n",
        "generated_audio = np.concatenate(generated_clips)\n",
        "ipd.display(ipd.Audio(data=generated_audio, rate=sample_rate)) # display audio widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random interpolation\n",
        "\n",
        "Lets now create an interpolation between two random points in latent space, and list to what that sounds like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_interp = create_latent_interp(intervals=100, z_dim=latent_dim)\n",
        "interpolation_clips = []\n",
        "\n",
        "for latent in latent_interp:\n",
        "    # Convert to tensor and reshape for RAVE input\n",
        "    latent = torch.tensor(latent)\n",
        "    # This changes the shape from (128) to (1,128,1)\n",
        "    latent = latent.unsqueeze(0).unsqueeze(2)\n",
        "    \n",
        "    # Generate audio clip and append to list\n",
        "    gen_audio_clip = model.decode(z)\n",
        "    gen_audio_clip = gen_audio_clip.reshape(-1).cpu().numpy()\n",
        "    interpolation_clips.append(gen_audio_clip)\n",
        "\n",
        "# Concatonate list of audio clips into one array\n",
        "generated_audio = np.concatenate(interpolation_clips)\n",
        "print(generated_audio.shape)\n",
        "ipd.display(ipd.Audio(data=generated_audio, rate=sample_rate)) # display audio widget"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load an audio file and listen to it\n",
        "We can load an audio file using librosa (`li`). `li.load` returns an array where every item corresponds to the amplitude at each time sample. You can convert from time in samples to time in seconds using `time = np.arange(0, len(input_data))/sample_rate`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_file = \"../media/sounds/368377__rmeluch__cello-phrase-5sec.wav\" \n",
        "input_data = li.load(input_file, sr=sample_rate)[0] # load input audio\n",
        "\n",
        "time = np.arange(0, len(input_data)) / sample_rate # to obtain the time in seconds, we need to divide the sample index by the sample rate\n",
        "plt.plot(time,input_data)\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.title(input_file.split(\"/\")[-1])\n",
        "plt.grid()\n",
        "\n",
        "ipd.display(ipd.Audio(data=input_data, rate=sample_rate)) # display audio widget"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform timbre transfer\n",
        "We can now load a pretrained model using `torch.jit.load` and encode the input audio into a latent representation.For the vintage model, we will be encoding our input audio into a latent space trained on 80h of \"vintage music\". We can then decode the latent representation an synthesise it. This will make the original sound as if it was \"vintage music\" (timbre transfer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.from_numpy(input_data).reshape(1, 1, -1) # convert audio to tensor and add batch and channel dimensions\n",
        "z = model.encode(x) # encode audio into latent representation\n",
        "\n",
        "# synthesize audio from latent representation\n",
        "y = model.decode(z).numpy() # decode latent representation and convert tensor to numpy array\n",
        "y = y[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
        "y = y[abs(len(input_data)- len(y)):] # trim to match input length --> for some reason the output is a bit longer than the input\n",
        "\n",
        "# save output audio\n",
        "output_file =f'{generated_path}/{input_file.replace(\".wav\", f\"_{pretrained_model}_generated.wav\").split(\"/\")[-1]}'\n",
        "sf.write(output_file,y, sample_rate)\n",
        "\n",
        "ipd.Audio(output_file) # display audio widget"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare the input and output sound wave and spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1, t1, Zxx1 = signal.stft(input_data, fs=sample_rate, nperseg=2048, noverlap=512)\n",
        "f2, t2, Zxx2 = signal.stft(y, fs=sample_rate, nperseg=2048, noverlap=512)\n",
        "\n",
        "fig, axs = plt.subplots(2, 2,figsize=(10,5), sharex=True)\n",
        "\n",
        "axs[0,0].plot(time,input_data)\n",
        "axs[0,0].set_ylabel(\"Amplitude\")\n",
        "axs[0,0].grid()\n",
        "axs[0,0].set_title(input_file.split(\"/\")[-1])\n",
        "axs[1,0].plot(time,y)\n",
        "axs[1,0].set_ylabel(\"Amplitude\")\n",
        "axs[1,0].set_xlabel(\"Time (seconds)\")\n",
        "axs[1,0].grid()\n",
        "axs[1,0].set_title(output_file.split(\"/\")[-1])\n",
        "\n",
        "axs[0,1].pcolormesh(t1, f1[:100], np.abs(li.amplitude_to_db(Zxx1[:100,:],\n",
        "                                                       ref=np.max)))\n",
        "axs[1,1].pcolormesh(t2, f2[:100], np.abs(li.amplitude_to_db(Zxx2[:100,:],\n",
        "                                                       ref=np.max)))\n",
        "axs[1,1].set_xlabel(\"Time (seconds)\")\n",
        "axs[0,1].set_title(\"STFT\")\n",
        "axs[0,1].set_ylabel(\"Frequency (Hz)\")\n",
        "axs[1,1].set_ylabel(\"Frequency (Hz)\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alter latent representation\n",
        "We can now modify the latent coordinates of the input file to alter the representation. We can start by adding a constant bias (a displacement) to the coordinates in the latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(z.shape) # the second dimension corresponds to the latent dimension, in this case, there's 8 latent dimensions\n",
        "\n",
        "d0 = 1.09  # change in latent dimension 0\n",
        "d1 = -3 \n",
        "d2 = 0.02\n",
        "d3 = 0.5 \n",
        "# we leave dimensions 4-8 unchanged\n",
        "\n",
        "z_modified = torch.clone(z) # copy latent representation\n",
        "# bias latent dimensions (displace each sample representation by a constant value)\n",
        "z_modified[:, 0] += torch.linspace(d0,d0, z.shape[-1])\n",
        "z_modified[:, 1] += torch.linspace(d1,d1, z.shape[-1])\n",
        "z_modified[:, 2] += torch.linspace(d2,d2, z.shape[-1])\n",
        "z_modified[:, 3] += torch.linspace(d3,d3, z.shape[-1])\n",
        "\n",
        "y_latent_1 = model.decode(z_modified).numpy() # decode latent representation and convert tensor to numpy array\n",
        "y_latent_1 = y_latent_1[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
        "y_latent_1 = y_latent_1[abs(len(input_data)- len(y_latent_1)):] # trim to match input length\n",
        "output_file = f'{generated_path}/{input_file.replace(\".wav\", f\"_{pretrained_model}_latent_generated_1.wav\").split(\"/\")[-1]}'\n",
        "sf.write(output_file,y_latent_1, sample_rate) # save output audio\n",
        "\n",
        "ipd.Audio(output_file) # display audio widget"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sinusoid manipulation to latent\n",
        "\n",
        "Instead of using a constant (a bias) to displace the representation of every sample in the latent space, we can use a function so that we \"navigate\" the latent space. For example, we can use a sinusoidal function that the representation oscillates around the original encoded one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_modified = torch.clone(z) # copy original latent representation\n",
        "\n",
        "# bias latent dimensions with a sinusoidal function at 440 Hz\n",
        "t = torch.linspace(0, z.shape[-1], z.shape[-1])\n",
        "for idx in range(0, z.shape[1]): # for each latent dimension\n",
        "    z_modified[:, idx] += torch.sin(440*2*np.pi*t)\n",
        "\n",
        "y_latent_2 = model.decode(z_modified).numpy() # decode latent representation and convert tensor to numpy array\n",
        "y_latent_2 = y_latent_2[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
        "y_latent_2 = y_latent_2[abs(len(input_data)- len(y_latent_2)):] # trim to match input length\n",
        "output_file = f'{generated_path}/{input_file.replace(\".wav\", f\"_{pretrained_model}_latent_generated_1.wav\").split(\"/\")[-1]}'\n",
        "sf.write(output_file,y_latent_2, sample_rate) # save output audio\n",
        "\n",
        "ipd.Audio(output_file) # display audio widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tasks\n",
        "\n",
        "**Task 1:** Run this code with the different pre-trained models to see the differences in the audio generated by the model.\n",
        "\n",
        "**Task 2:** Load in your own audio track (your favourite song or a recording you have) and do the timbre transfer with it. \n",
        "\n",
        "Then move onto `interactive-audio-generation.py` to see how we can use these RAVE models for realtime interactive audio generation. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "rave",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "b569cc9be53083e787d8b1313a26e0731e8a98a84352215d2f1ca78ae62b88e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
