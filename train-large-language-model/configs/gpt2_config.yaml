model_config:
  n_layer: 12
  n_head: 12
  n_embd: 768
  block_size: 1024
  vocab_size: 50304
  dropout: 0.2
  bias: False

---

training_config:
  batch_size: 8
  learning_rate: 0.0001
  min_lr: 0.00005
  max_iters: 600000
  lr_decay_iters: 600000
  warmup_iters: 200
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  weight_decay: 0.1
  decay_lr: True
  init_from: 'gpt2'