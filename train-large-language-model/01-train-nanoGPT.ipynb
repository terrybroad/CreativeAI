{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a GPT model from scratch\n",
    "\n",
    "Here we are going to train a GPT model from scratch (babyGPT) using characters as tokens. This code can also be used to fine-tune GPT-2 models. \n",
    "\n",
    "The code in this week is based on [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT), a really simple and lightweight implementation of a large language model. This code was chosen as the most accessible codebase where you can inspect and understand the code behind transformers, as well as easily train and customise your own simple GPT models. \n",
    "\n",
    "The code here has been modified a bit to make it simpler, more Pythonic, and more easy for you to adapt for your own project. But the core of the technical implementation of the LLM in `nanoGPT/model.py` is largely unchanged. When this is training **spend some time looking at the model implementation code** and try to see what bits of code correspond to the components of a transformer model as discussed in the lecture.\n",
    "\n",
    "Before running this you will need to run `00-prepare-dataset.ipynb` (that you can easily customise to your own text dataset). After running this you can use your model to do some generation in `02-generate-with-nanoGPT.ipynb`. \n",
    "\n",
    "Unlike previous weeks to model and training hyperparemeters are specified in a separate YAML config file, which is a hand format for writing human readable code configs that get read as python dictionaries (see `configs/` and `nanoGPT/config.py` for more details). This is to make it easier to switch back and forth between models and training params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from yaml.loader import SafeLoader\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from nanoGPT.model import GPT\n",
    "from nanoGPT.config import ModelConfig, TrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# I/O\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "\n",
    "# train a miniature character-level shakespeare model\n",
    "# good for debugging and playing on macbooks and such\n",
    "# if fine-tuning a gpt model you want to make these bigger\n",
    "eval_interval = 50 # keep frequent because we'll overfit\n",
    "eval_iters = 50\n",
    "log_interval = 10 # don't print too too often\n",
    "\n",
    "# we expect to overfit on this small dataset, so only save when val improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training config and dataset\n",
    "\n",
    "In the folder `configs` there are a number of yaml files with different configs for the model and training of our GPT models. You can look in there if you want to see how these are set up. By default this notebook will use `baby_gpt_config.yaml` which will use characters as tokens so we can train a simple and small GPT model for educational purposes. For good quality results though, you will want to fine-tune a pretrained GPT2 model using the second config set here. \n",
    "\n",
    "This code has been designed for you to train these on whatever custom text dataset you want, using the code in `00-prepare-dataset.ipynb`. Just use the *chars* or *tokens* dataset depending on whether you want to train a babyGPT or GPT-2 model. \n",
    "\n",
    "##### Train a babyGPT character model from scratch:\n",
    "\n",
    "This configuration will train a babyGPT model from scratch at the character level (similar to CharRNN from last term). This is designed to be small enough to be trained from scratch on a laptop cpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'configs/baby_gpt_config.yaml'\n",
    "dataset_dir = '../data/class-datasets/text-datasets/shakespeare_chars'\n",
    "ckpt_dir = 'ckpt/shakespeare-char'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-tune a GPT2 model on a custom dataset (GPU recommended):\n",
    "\n",
    "This configuration will automatically load a pretrained GPT-2 Model from huggingface transformers which you can fine-tune on your custom tokenised dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path = 'configs/gpt2_config.yaml'\n",
    "# dataset_dir = '../data/class-datasets/text-datasets/shakespeare_tokens'\n",
    "# ckpt_dir = 'ckpt/shakespeare-gpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and training hyperparameters\n",
    "\n",
    "Here we will load our Model and Training hyperparameters into our ModelConfig and TrainingConfig classes, which we will call `m` and `t`. This is different to the original nanoGPT code and was built in order to make something that was more robust and Pythonic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    m_dict, t_dict = list(yaml.load_all(f, Loader=SafeLoader))\n",
    "\n",
    "m = ModelConfig.from_dict(m_dict['model_config'])\n",
    "t = TrainingConfig.from_dict(t_dict['training_config'])\n",
    "print(m.__dict__)\n",
    "print(t.__dict__)\n",
    "tokens_per_iter = gradient_accumulation_steps* t.batch_size * m.block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n",
    "Here we load in our data from our pre-calculated binaries `.bin` of our list of token indicies for our text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap(os.path.join(dataset_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(dataset_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - m.block_size, (t.batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+m.block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+m.block_size]).astype(np.int64)) for i in ix])\n",
    "    if device == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise model\n",
    "\n",
    "Here we initialise our model. It gets a bit complicated as we want to override the params in our ModelConfig `m` if there is a conflict with the params listed in a saved checkpoint that we load in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init these up here, can override if t.init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(dataset_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "    \n",
    "# model init\n",
    "if t.init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    m.vocab_size = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    model = GPT(m)\n",
    "elif t.init_from == 'resume':\n",
    "    print(f\"Resuming training from {ckpt_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(ckpt_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    m = ModelConfig.from_dict(checkpoint_model_args)\n",
    "    # create the model\n",
    "    model = GPT(m)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif t.init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {t.init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=m.dropout)\n",
    "    model = GPT.from_pretrained(t.init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    m = ModelConfig.from_dict(model.config.__dict__)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if m.block_size < model.config.block_size:\n",
    "    model.crop_block_size(m.block_size)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup coure objects\n",
    "\n",
    "Here we setup our core objects like our optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(t.weight_decay, t.learning_rate, (t.beta1, t.beta2), device)\n",
    "if t.init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions\n",
    "\n",
    "Some utility functions to help us estimate the loss and get the learning rate (if we set it to gradually step down over training)L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it, t):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < t.warmup_iters:\n",
    "        return t.learning_rate * it / t.warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > t.lr_decay_iters:\n",
    "        return t.min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - t.warmup_iters) / (t.lr_decay_iters - t.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return t.min_lr + coeff * (t.learning_rate - t.min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Here is our training loop. The code is a bit more complex here than we have sesn before. This is largely because there is quite a bit of boilerplate code which can be used to make training more efficient on NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num, t) if t.decay_lr else t.learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': m.__dict__,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': model.config.__dict__,\n",
    "                    'dataset': dataset_dir\n",
    "                }\n",
    "                print(f\"saving checkpoint to {ckpt_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(ckpt_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if t.grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), t.grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = model.estimate_mfu(t.batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > t.max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "**Task 1:** Run the code here and to train a babyGPT model using characters as tokens on the shakespeare dataset or on a custom dataset of your choice. While training is happening, do task 2:\n",
    "\n",
    "**Task 2:** Go to `nanoGPT/model.py` and look through the code, which lines of code are responsible for:\n",
    " - Token embedding\n",
    " - Positional embedding\n",
    " - Self attention\n",
    " - Mask for self attention\n",
    " - Layer normalisation\n",
    " - The fully connected layers\n",
    " - The complete transformer block\n",
    " - Softmax probabilities of outputs\n",
    "\n",
    "**Task 3:** Once your model has trained use it to generate some outputs in `02-generate-with-nanoGPT.ipynb`. How does your model perform? You can resume training from a saved checkpoint if you want to restart and resume training of your model. \n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "**Task A:** Change the [config](#set-training-config-and-dataset) for this code to fine-tune a pretrained GPT2 model on the tokenised shakespeare dataset (or other dataset of your choice) and run it. **Disclaimer** you will probably need an NVIDIA GPU and a lot of time to do this well.\n",
    "\n",
    "**Task B:** Look at the template for a config in `configs`, can you make your own custom GPT architecture by setting new parameters in the config? Maybe you could train a babyGPT model from scratch using the GPT2 tokeniser instead of on characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
